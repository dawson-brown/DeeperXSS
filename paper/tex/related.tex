Mokbal \textit{et al.} created a multilayer preceptron (MLP) model for detecting XSS both in dynamic webpages and URLs \cite{mokbal2019mlpxss}. Their approach, called MLPXSS, has three main pillars: data scraping, feature extraction, and an artificial neural network ANN. Their model is meant to deal with both dynamic webpages and malicious URLs. Their feature extraction level has 3 modules to extract HTML-based features, Javasript-based features, and URL-based features. The HTML module tokenizes tags, attributes and events--focusing on things that trigger Javascript execution (like \textit{href} or \textit{onclick}). The Javascript module parses and tokenizes Javascript code that is pulled from a webpage. There are various ways to include Javascript in a page like script tags, \textit{onclick} and \textit{onsubmit} calls, \textit{href}, etc... Lastly they tokenize potentially malicious parts of URLs, like HTML properties, tags, some keywords (\textit{login}, \textit{signup}), \textit{document} references, and various special characters like `$<$', `$>$' and `$/$'. The MLP is trained on token streams with a sigmoid output layer. Their perceptron had precision, f-measure, and accuracy all in excess of 99\% \cite{mokbal2019mlpxss}.

Zhang \textit{et al.} propose a dual Gaussian mixture model (GMM) approach that trains two seperate GMMs models (one for benign and one for malicious) and then combines their outputs to make a prediction. Additionaly, they train the models on both the URL and the server response to the URL in an attempt to get richer features \cite{zhang2019cross}. To preprocess the URLs, the decode, tokenize, and train a word2vec model to retrieve a vector representation of each token. Their tokenization approach is very similar to that of DeepXSS and MPLXSS, they however inclue the domain and path for the benign GMM. They are however generalized to simply `domain' and `path' \cite{zhang2019cross}. Their reasoning for this is that containing just a domain and path is characteristic of a benign URL, whereas an XSS URL is characterized by its maliciouly constructed parameters and not the presence of a domain and path \cite{zhang2019cross}. Their models can be trained on requests, responses, or both. They reason that in many cases, benign requests contain no XSS tokens, which isn't very interesting, however responses contain useful features for both XSS and none-XSS tokenization. Their multi-stage dual GMM using both responses and requests greatly improved classification \cite{zhang2019cross}.

Goswami \textit{et al.} propose a attribute clustering technique to perform unsupervised grouping of malicious and benign scripts. They're feature extraction is wholly different from DeepXSS and other deep learning classifiers. They propose 15 features that characterize malicious and benign scripts creating a 16-dimensional vector for each script (including class) \cite{goswami2017unsupervised}. These features are meta-features like length of the script, number of strings and the average string length, number of methods, number of unicode and hex characters, among others. These features are then min-max normalized before clustering \cite{goswami2017unsupervised}. Their algorithm was able to achieve an accuracy in excess of 98\% \cite{goswami2017unsupervised}.  

The authors in \cite{afzal2021deeplearning} focus on classifying broadly defined malicious URLs sent through email or over social networks.  In this case a malicious URL is any URL that could result in harm to the user visiting it.  They propose a hybrid deep-learning approach called \textit{URLdeepDetect} to extract semantic features from URLs to classify them as either benign or malicious.  The preprocessing stages tokenizes various parts of the URL before applying word-level embedding through Word2Vec.  The embedded tokens are then fed to an LSTM where samples are classified based on LSTM outputs or k-means clustering.  This paper claims 98.3\% accuracy for LSTM classification of malicious URLs and 99.7\% accuracy with k-means clustering.  The authors claim that the success of their approach is in part due to the Word2Vec token embedding and maintaining URL sequence as it provides the model with more semantic information for classification.

The work done in \cite{vishnu2014prediction} explores XSS detection using three machine learning algorithms: Na√Øve Bayes, Support Vector Machine, and J48 Decision Tree.  This paper attempts to detect reflected XSS, persistent XSS, and DOM based XSS meaning it requires web page scripts as well as URLs.  The malicious URLs and scripts were collected from the XXSed \cite{xssed} project and the benign samples were collected from the Dmoz open directory project \cite{dmoz}.  This work, however, only uses five features for scripts (e.g., number of characters, request for cookie, etc.) and seven features for URLs (e.g., number of characters, presence of script tags, etc.) rather than extracting tokens to represent the entire URL or script.  Each model was then subjected to 10-fold cross validation and the results were compared.  J48 performed the best based on features from both URL and JavaScript achieving a 99\% true positive rate and 99\% precision.  The authors also found that discretized attributes provided the best classifier results for J48.

Detecting XSS attacks on social networking sites is the primary focus of \cite{rathore2017xss}.  The authors propose an approach consisting of feature identification, web page collection, feature extraction, building a training dataset, and using a machine learning algorithm to classify web pages as XSS or non-XSS.  This approach extracts features from URLs, HTML tags, and the host social networking site.  The features of these web pages are very coarse-grained, consisting of things like the maximum size of URLs, and counts of harmful keywords.  The authors then train ten classifiers with a RandomForest classifier achieving 97.7\% precision and 97.1\% recall.  They suggest future work to enhance the feature set and apply more machine learning algorithms such as deep learning.