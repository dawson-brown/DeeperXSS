Our implementation mimics the original paperâ€™s use of 10-fold cross validation wherein the model is divided into ten equal subsets.  The model is trained ten times for ten epochs.  Each subset is used as a validation dataset exactly once, with the remaining data serving as training data.  The final results are an average of the ten models.  This process was repeated for all eight of the models we tested.
The confusion matrix we show in \ref{conf-mat} is identical to the one used in DeepXSS \cite{fang2018deepxss}. $T_p$ (True Positive) indicates an actual XSS sample predicted to be XSS. $T_n$ (True Positive) indicates a benign sample predicted to be benign. $F_p$ are benign samples classified as malicious and $F_n$ is a malicious sample classified as benign.  These definitions are used to inform our key metrics, precision, recall, F1, and accuracy.  The calculations for these are shown in the formulae below.

\begin{table}
\begin{center}
\begingroup
\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{tabular}{|| c | c | c ||} 
    \hline
    Prediction & XSS & Not XSS \\ 
    \hline\hline
    \textbf{Predicted XSS} &  $T_p$ & $F_p$ \\ 
    \hline
    \textbf{Predicted Not XSS} & $F_n$ & $T_n$ \\
    \hline
\end{tabular}
\endgroup
\caption{\label{conf-mat}Confusion Matrix}
\end{center}
\end{table}


\begin{align*}
    &\text{Precision} = \frac{T_p}{T_p + F_p} \\[10pt]
    &\text{Recall} = \frac{T_p}{T_p + F_n} \\[10pt]
    &\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\\[10pt]
    &\text{Accuracy} = \frac{T_p + T_n}{T_p + T_n + F_p + F_n}
\end{align*}

Tables \ref{comparison} and \ref{type:comparison} outline the comparison between our various models using both token values and token types respectively.  Our results are ultimately very similar to DeepXSS \cite{fang2018deepxss} for the primary models with the softmax output layer.  This indicates that our model was an acceptable recreation of the original. All other models also perform well with the very notable exception of the model with the randomly initialized embedding layer.

\begin{table}
\begin{center}
\begingroup
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{tabular}{|| c | c | c | c | c ||} 
    \hline
    Model & Precision & Recall & F1 & Accuracy \\ 
    \hline\hline
    \textbf{DeepXSS} &  0.995 & 0.979 & 0.987 & n/a \\ 
    \hline
    \textbf{DeeperXSS:softmax} & 0.989 & 0.973 & 0.981 & 0.981 \\
    \hline
    \textbf{DeeperXSS:sigmoid} & 0.988 & 0.976 & 0.982 & 0.983 \\
    \hline
    \textbf{DeeperXSS:no embed} & 0.991 & 0.956 & 0.973 & 0.975 \\
    \hline
    \textbf{DeeperXSS:random embed} & 0.099 & 0.097 & 0.098 & 0.56 \\
    \hline
\end{tabular}
\endgroup
\caption{\label{comparison}Token Value Model Comparison.}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begingroup
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{tabular}{|| c | c | c | c | c ||} 
    \hline
    Model & Precision & Recall & F1 & Accuracy \\ 
    \hline\hline
    \textbf{DeepXSS} &  0.995 & 0.979 & 0.987 & n/a \\ 
    \hline
    \textbf{DeeperXSS:softmax} & 0.989 & 0.978 & 0.981 & 0.982 \\
    \hline
    \textbf{DeeperXSS:sigmoid} & 0.987 & 0.977 & 0.982 & 0.983 \\
    \hline
    \textbf{DeeperXSS:no embed} & 0.983 & 0.955 & 0.969 & 0.97 \\
    \hline
    \textbf{DeeperXSS:random embed} & 0.089 & 0.091 & 0.08 & 0.555 \\
    \hline
\end{tabular}
\endgroup
\caption{\label{type:comparison}Token Type Model Comparison.}
\end{center}
\end{table}