Cross-site scripting (XSS) attacks persist as a major issue for Web applications despite their root causes being well understood. XSS is a vulnerability by which attackers can inject malicious code into an HTML page which is then received and executed by a victim's browser. With ever more sensitive data being transacted on the Web, the personal and industrial consequences of XSS are ever increasing \cite{andreeva2016industrial}. XSS can be split into three broad categories: persistent, reflected, and DOM \cite{gupta2017cross}. Persistent XSS is when an attacker manages to save malicious code on a server to be served later to victims; reflected occurs when a server echos back a portion of a request--if an attacker can get unsanitized malicious code to be echoed to a victim's machine (by having the victim click a malicious link) the code will be executed in the browser; And lastly DOM is when the attack is executed by modifying the DOM of the victim's browser \cite{gupta2017cross}. The Open Web Application Security Project (OWASP) consistently ranks XSS in the top ten vulnerabilities on the Web, including in their most recent 2021 survey (however XSS was lumped into the generic 'injection' category in 2021 instead of having its own category) \cite{owasp}. 

For this reason, there exists a large body of work aimed at automating the detection and prevention of XSS attacks; this includes approaches using Machine Learning techniques to detect both attacks and vulnerabilities. DeepXSS is an LSTM classifier that is meant to detect XSS payloads in reflected XSS attacks; it was designed by Fang \textit{et. al} and is purported to have very high precision and recall rates \cite{fang2018deepxss}. We want to more clearly outline and comment on the strengths and weaknesses of their architecture as well as replicate their purported results.\cite{fang2018deepxss}. 

The primary motivation for this work is to verify Fang \textit{et. al}'s DeepXSS method of detecting cross-site scripting (XSS) attack payloads \cite{fang2018deepxss}. Given the significance of XSS attacks on the Web, there is a need to scrutinize detection and prevention techniques to better understand what works and what doesn't. We found that DeepXSS lacked detail and failed to address key questions related to their work and so required a deeper investigation. Given DeepXSS's promising results, it would be very useful to address the lack of detail, and attempt to replicate the methods used by the authors.  In verifying these results we could further our understanding of why DeepXSS was so effective (or why it was not as effective as it seemed) and apply those lessons to future research. Our main contributions are as follows:

\begin{enumerate}
    \item A custom built URL decoder
    \item An extension of DeepXSS's tokenization technique
    \item A successful recreation of DeepXSS's Word2Vec application
    \item A successful recreation of DeepXSS's LSTM classifier results
    \item Seven additional LSTM models that vary in their input, embedding and/or output to compare and contrast some of the choices made in DeepXSS
    \item A public Github repository with all our code and data \footnote[1]{https://github.com/dawson-brown/DeeperXSS}
\end{enumerate}

The rest of this paper is organized as follows: first we give a brief overview of DeepXSS highlighting its limitations in terms of reproducibility, then we outline our DeeperXSS approach where we recreate (with some modifications) the DeepXSS architecture, then we evaluate our models using 10-fold cross validation, we then go over some related work that factored into our decision making when trying to recreate this work, lastly we give a brief discussion and conclusion.