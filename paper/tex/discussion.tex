This approach is entirely dependent on the scope and correctness of the decoder. The filter evasion techniques used for XSS can be extremely complicated and can include multiple and mixed encodings with near arbitrary white space \cite{xsscheat}. To demonstrate this sensitivity, we applied the standard blacklist/whitelist sanitizer \textbf{bleach} to both the decoded and non-decoded malicious URLs to test the relative efficacies \cite{bleach}. On non-decoded data, the sanitizer only cleaned 62\% of malicious samples, while on the decoded data it cleaned 91\% of samples, clearly showing the importance of decoding. Furthermore, new evasion techniques are continually being discovered as well. The decoder has to be sophisticated enough to handle the evasion techniques of novel XSS payloads to be useful in practice. That said, a good decoder will be hard to evade and probably only a negligible number of XSS payloads will get past it, on the other hand, given the severity of the consequences of some XSS attacks, no number is negligible. 

As with all machine learning, this approach is very sensitive to the preprocessing of the data and the feature extraction. In our case, this is the tokenization step. There have been many different tokenization procedures proposed; often they exclusively looked for tokens that would indicate a malicious URL and largely ignored the benign segments of a URL \cite{fang2018deepxss}\cite{mokbal2019mlpxss}. This approach means that many benign URLs contain no tokens \cite{zhang2019cross}. This sort of approach requires the designer of the tokens to judge what kinds of strings characterize malicious URLs and only tokenize those which can be hard to do well. To address this, perhaps both the URL and the server's response to it can be tokenized which gives a much richer set of features and would help fill out the empty URLs \cite{zhang2019cross}. This has the drawback of having to interact with the server for each URL which adds overhead--ideally the URL provides sufficient information. In our approach, we had a more general tokenization approach that aimed at tokenizing URLs and not just malicious URLs. The idea being that the machine learning algorithm can be left to figure out what sort of strings and patterns characterize malicious URLs without much input from us. This means benign URLs aren't empty and the server response is not required. That said, there are likely benefits to including special strings that give a strong indication of an XSS payload. 

With regards to the models themselves, our results indicate that we have achieved a successful recreation of the original DeepXSS paper validating claims of highly effective XSS classification \cite{fang2018deepxss}. This is a reasonable conclusion given the empirical effectiveness of deep learning methods for classification problems. DeepXSS does have slightly higher metrics than DeeperXSS and the discrepancies can likely be explained by a few key limitations. We were unable to use the exact dataset used for DeepXSS, we lacked specific details about the Word2Vec implementation and the exact nature of the tokens it was applied to, and we lacked details as to how the embedded tokens were used as inputs to the model. With more time we likely could have improved our results with more training epochs, tuning hyperparameters like the dropout rate, and more rigorous data preparation to remove unsuitable samples.

Interestingly, the results trained with token type and token value are almost identical. The models without token embeddings also perform comparably well to the original.  This calls into question the significance of the Word2Vec step that is cited as a primary contribution of the original work. At best the word embeddings provide a marginal boost to the modelâ€™s metrics. One explanation could be that the model does not heavily rely on semantic information surrounding the tokens but rather focuses on features like the presence of specific tokens, the length of the URL, and the sequence particular tokens occur in. This said, a properly trained CBOW embedding layer massively outperformed a randomly initialized embedding layer trained alongside the classifier. Training the CBOW model was necessary in order to produce usable word embeddings. Future work should focus on isolating the most important features for classification.